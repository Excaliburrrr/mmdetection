# 目标检测笔记

## 一、R-CNN发展史

### 1、R-CNN (Region with CNN features)

R-CNN利用候选区域（Region Proposal）方法创建了约2000个region proposal。这些区域被转换为固定大小的图像，并分别馈送到卷积神经网络中。该网络架构后面会跟几个全连接层，以实现目标分类并提炼边界框。

![RCNN](/home/vansikey/Pictures/RCNN.png)

R-CNN的算法流程如下：

> 1、输入图像
>
> 2、对每张图像生成1K～2K个候选区域（采用 Selective Search 方法）
>
> 3、对每个候选区域，使用CNN提取特征
>
> 4、将特征送入训练好的每一类SVM分类器，判别是否属于该类
>
> 5、使用回归器精细修正候选框位置

下面展开进行介绍：

#### 1、生成候选区域

使用 Selective Search （选择性搜索）方法对一张图像生成约2000~3000个候选区域，基本思路如下：

1.  使用一种过分割手段，将图像分割成小区域

2. 查看现有小区域，合并可能性最高的两个区域，重复直到整张图合并成一个区域位置。优先合并以下区域：

   - 颜色（颜色直方图）相似的
   - 纹理（梯度直方图）相似的
   - 合并后总面积小的
   - 合并后，总面积在其BBOX中所占比例大的

   在合并时须保证合并操作的尺度较为均匀，避免一个大区域陆续 ”吃掉“ 其他小区域，保证合并后形状规则。

3. 输出所有曾经存在过的区域，即所谓候选区域

#### 2、特征提取

将每一个候选区域归一化成统一尺寸如 $277\times277$  。分别送入CNN中提取特征。

#### 3、类别判断

对每一类目标，使用一个线性SVM二分类器进行判断，输入为CNN输出的4096维特征，输出是否属于此类。

#### 4、位置精修

目标检测的衡量标准是重叠面积：许多看似准确的检测结果，往往因为候选框不够准确，重叠面积很小，故需要一个位置精修步骤，对于每一类，训练一个线性回归模型去判断这个框是否框的完美。

R-CNN将深度学习引入检测领域后，一举将PASCAL VOC上的检测率从35.1%提升到53.7%。

### 2、Fast R-CNN

继2014年的R-CNN推出之后，Ross Grishick在2015年推出Fast R-CNN，构思精巧，流程更为紧凑，大幅提升了目标检测的速度。

Fast R-CNN和R-CNN相比，训练时间从84小时减少到了9.5小时，测试时间从47秒减少到了0.32秒，并且在PASCAL VOC 2007上测试的准确率相差无几，约在66%-67%之间。



![img](/home/vansikey/Pictures/fast_rcnn_vs_rcnn.png)

#### Fast R-CNN V.S. R-CNN

Fast R-CNN主要解决R-CNN的以下问题：

> 1. 训练、测试速度慢
>
>    R-CNN 的一张图像内候选框之间存在大量重叠，逐一对Region Proposals提取特征操作是非常冗余的。而Fast R-CNN是将整张图像归一化后直接送入CNN中得到整张图的feature map，紧接着送入这幅图像上提取出的候选区域，根据CNN的尺度不变性（一定的不变性），在feature map上找到region proposal的对应位置。由此实现了对候选区域共享卷积过程。
>
> 2. 训练所需空间大
>
>    R-CNN中独立的分类器和回归器需要大量特征作为样本。Fast R-CNN把类别判断和位置精调统一用CNN实现，不需要额外储存。

下面进行详细介绍：

#### 网络结构

![1562074709656](/home/vansikey/.config/Typora/typora-user-images/1562074709656.png)

__1. 在特征提取阶段__ ，输入图片的尺寸不同会导致feature map尺寸也不同，因此不能直接通过FC进行分类。但是在Fast R-CNN中，作者提出了一个叫做 **ROI Pooling** 的网络层，这个网络层可以把不同大小的输入映射到一个固定尺度的特征向量，ROI Pooling将每个候选区域均匀分成 $M\times N$ 块，对每块进行max pooling。如此一来，将特征图上大小不一样的候选区域转变成大小统一的数据，则可以送入FC层进行后面的任务。这样虽然由于输入的图片尺寸不同使得到的featur map尺寸也不同，但是可以通过这个神奇的ROI Pooling层，对每个region都提取一个固定维度的特征表示 ($7\times 7$的张量)。

__2. 在分类回归阶段__，在R-CNN中，先生成Region Proposal，然后在将每一个Proposal通过CNN分别提取特征，之后再用SVM进行分类，最后再做回归 (bbox regression) 得到具体位置。而在Fast R-CNN中，作者巧妙的把最后的bbox regression也放进了神经网络内部，与区域分类合并成了一个multi-task模型，如上图所示。实验表明，这两个任务可以共享卷积特征，并且相互促进。

#### ROI Pooling

ROI Pooling 层能实现training和testing的显著加速，并提高检测的准确率。该层有两个输入：

* 从具有多个卷积和池化的深度网络中获得的固定大小的feature maps；
* 一个表示所有ROI的N×5的矩阵，其中N表示ROI的数目。第一列表示图像的index，其余四列表示ROI的左上角和右下角的坐标l；

ROI Pooling具体操作如下：

1. 根据输入的image，将ROI映射到feature map对应的位置上；
2. 将映射后的区域划分为相同大小的sections（sections数量与输出的维度相同）；
3. 对每个sections进行max pooling操作；

这样我们就可以从不同大小的方框得到固定大小的相应的feature maps。值得一提的是输出的feature maps的大小不取决于ROI和原CNN输出的feature maps的大小。ROI Pooling最大的好处就在于极大地提高了处理速度。

#### ROI Pooling Example

考虑一个8×8大小的feature map，一个ROI，以及设定的输出大小为2*2。

1. 输入的固定大小的feature map

   ![1562078531034](/home/vansikey/.config/Typora/typora-user-images/1562078531034.png)

2. Region Proposal 投影之后的位置（左上角，右下角）: (0, 3), (7, 8)。

   ![1562078651806](/home/vansikey/.config/Typora/typora-user-images/1562078651806.png)

3. 将其划分为(2×2)个sections，我们可以得到：

   ![1562078712495](/home/vansikey/.config/Typora/typora-user-images/1562078712495.png)

4. 对每个section做max pooling，可以得到：

   ![1562078751387](/home/vansikey/.config/Typora/typora-user-images/1562078751387.png)

#### ROI Pooling总结：

> 1. 用于目标检测任务；
> 2. 允许对CNN中的feature map进行reuse；
> 3. 可以显著加速training和testing速度；
> 4. 允许端到端的训练目标检测系统。

#### 贡献

Fast R-CNN很重要的一个贡献是成功地让人们看到了Region Proposal+CNN 这一框架的实时检测的希望，原来多类别检测真的可以在保证准确率的同时提升处理速度。

### 3、Faster R-CNN

继2014年推出R-CNN，2015年推出Fast R-CNN之后，目标检测界领军人物Ross Grishick团队在2015年又推出一力作：Faster R-CNN，使得简单网络目标检测速度达到17 fps，在PASCAL VOC 上准确率为59.9%，复杂网络达到5 fps，准确率为78.8%。

在Fast R-CNN 上还存在着瓶颈问题：Selective Search（选择性搜索）。使用Selective Search去找到所有的Region Proposals是非常耗时的，那我们有没有一个更加高效的方法求出这些候选框呢？

在Faster R-CNN中加入了一个提取边缘的神经网络，也就是说找候选框的工作也交给了神经网络来做了。这样，目标检测的四个基本步骤（候选区域生成，特征提取，分类，位置精修）终于被统一到一个深度网络框架之内。如下图所示：

![1562117499268](/home/vansikey/.config/Typora/typora-user-images/1562117499268.png)

Faster R-CNN可以简单的看成是 ”区域生成网络+Fast R-CNN“ 的模型，用区域生成网络（Region Proposal Network，简称RPN）来代替Fast R-CNN中的Selective Search方法。

#### 1.网络结构

![1562070035168](/home/vansikey/.config/Typora/typora-user-images/1562070035168.png)

Faster R-CNN的结构是复杂的，因为其中有几个移动部件，在这里我们对整个网络进行分解，分解为一下几个部分：Backbone（主干网络）、Anchor Heads（RPN）、ROI_extractor（ROI Pooling）、Bbox_heads。先从宏观上对整个网络的前向过程做一个介绍：

> 1. 首先输入图片的表示为 维度为$H\times W\times D$ 的张量，
> 2. 将输入图片的张量送入Backbone网络（VGG、ResNet等，该网络已经在ImageNet数据集上进行了预训练）提取特征图 feature map，
> 3. 将提取的featrue map送入RPN（Region Proposal Network）用于生成可能包含objects的**预定义数量的区域** （Proposal）
> 4. 将从RPN层出来的Proposals送入ROI extractor层进行ROI Pooling操作，得到统一大小的feature maps（7×7×256）
> 5. 将RoI extractor层中得到的统一大小的feature maps送入Bbox_heads，中进行回归和分类操作得到具体的物体框和框内物体的类别。

接下来对一一对其做详细介绍：

#### Backbone

正如上面所说，Faster R-CNN第一步是采用基于分类的任务（如，ImageNet）的CNN模型作为特征提取器。Faster R-CNN最早是采用在ImageNet上训练的ZF和VGG，其后出现了很多其他权重不同的网络。如，MobileNet是一种小型效率高的网络结构，仅有3.3M参数；而，ResNet-152的参数达到了60M，2017年的DenseNet在提高了结果的同时，降低了参数数量。

* VGG
* ResNet
* MobileNet
* HRnet
* ....



#### RPN (Region Proposal Networks)

通过上述介绍可知，Faster R-CNN与Fast R-CNN最大的区别就是提出了一个叫**RPN**的网络，专门用来产生Region Proposal，在RPN中主要实现了三件事情：

> 1. 对feature map上的每个cell都产生9个Anchors（3种不同尺度，3种不同长宽比）
> 2. 将feature map分别通过两个卷积层得到每个anchor相对于ground-truth bbox的offsets和是否包含有物体的confidence，并且对每个anchor根据与gt bbox的IoU进行分类（对于每一个anchor，找到一个与其有最大IoU的gt bbox，并与设定的阈值进行比较，判断其是前景框还是背景框，为了保证每一个gt bbox都有一个anchor去回归，还需要对每一个gt bbox都找一个与其有最大IoU的anchor并将该IoU与设定的最低IoU阈值进行比较，如果大于阈值则为fg anchor反之为bg anchor，具体的比较规则会在下面详细讲解）。
> 3. 对于所有分为前景的anchor，根据对应的offsets解码成bbox，然后通过nms（非极大值抑制）操作，去掉其中重叠率比较高的那些bbox，保留下来的bbox则为RPN网络最后输出的Proposal。



接下来我们针对以三点进行详细的解释：

##### **1、Anchor的解释**：



![1562125074698](/home/vansikey/.config/Typora/typora-user-images/1562125074698.png)

在Faster R-CNN中由于摒弃了之前的Selective Search的方式，而是通过设定一系列固定尺寸形状的anchor boxes来产生region proposal，相比于Selective Search，这样平铺anchor的方法更加的有效率。

在上图中，红色的框是 $3\times3$ 的卷积核在一张feature map上进行滑窗的操作过程，**注意**这里的得到的anchor bbox的坐标是原图像素空间中的，而不是feature map上的。

现在，我们假设feature map的尺寸为 $W\times H\times C$ (如 $13\times13\times256$)，在这样的feature map上使用滑窗的操作，当前滑窗的中心点在原像素空间点的映射点就成为anchor，并且以anchor为中心去生成K个anchor boxes（论文中K=9）。

##### **2、回归与分类** 

* 分类：对平铺了anchor的feature map利用一个size为1x1的conv(in_channels=256, out_channels=2K)进行卷积，得到的特征向量的长宽与原feature map相同，但通道数为2K，分别代表了K个proposals的Object的confidence；

* 回归：同样对平铺了anchor的feature map利用一个size为1x1的conv(in_channels=256, out_channels=4K)进行卷积，得到的特征向量的长宽与原feature map相同，但通道数为4K，分别代表了K个proposals的长宽及中心点坐标相对于原anchor bboxes的偏移量。

  其中对于边框回归，采用如下4个坐标的参数化：
$$
  t_x=(x-x_a)/w_a， t_y=(y-y_a)/h_a，
$$
  
$$
  t_w = log(w/w_a)， t_h=log(h/h_a)，
  $$
  
  $$
  t_x^* = (x^*-x_a)/w_a， t_y^*=(y-y_a)/h_a，
  $$
  
  $$
  t_w^*=log(w^*/w_a)， t_h^*=log(h^*/h_a)，
  $$
  
  其中网络预测量为 $t_x, t_y, t_w, t_h$ ，anchor bbox与gt_bbox的实际偏移量为 $t_x^*, t_y^*, t_w^*, t_h^*$ 。

##### 3、Anchor target

为了计算RPN层中回归与分类的的loss，需要为每一个anchor都分配一个gt_bbox_target和gt_label_target。由于在RPN阶段只对anchor分前景与背景，不区分具体类别，故gt_label_target为0, 1标签，将为anchor分配类别标签的过程称之为**anchor target** 。规则如下：

> ​	在anchor target过程中，是根据**最初平铺在feature map上的anchor bbox（后面简称为default bbox）**与gt_bbox之间的IoU的值来决定为anchor分配哪类标签(positive or negative or ignore)，首先设定三个阈值：pos_iou_thr=0.7， neg_iou_thr=0.3， min_pos_iou=0.3，分以下两步来进行：
>
> 1. 对每一个default bbox ，找到与其有最大IoU的gt_bbox，即该IoU为max_IoU；
>    * 若，max_IoU > pos_iou_thr， 则将该default bbox分为正样本框 (positive，也即前景框)，其gt_bbox_target为对应的gt_bbox，gt_label_target为对应的1 。
>    * 若，neg_iou_thr > max_IoU > pos_iou_thr， 则不计算该default bbox的loss，即该default bbox分为ignore。
>    * 若，max_IoU < neg_iou_thr，则将该default bbox分为负样本框 (negative，也即背景框)，gt_label_target为0 。
> 2. 经过1.之后，可能存在部分gt_bbox没有找到与之对应的default bbox；为保证每个gt_bbox都能存在至少一个default bbox去回归，还需要对每个gt_bbox都找一个与其有最大IoU的default bbox，记该IoU为gt_max_IoU；
>    * 若，gt_max_IoU > min_iou_thr，则将该default bbox分为正样本框，其gt_bbox_target为对于的gt_bbox，其gt_bbox_label为1；
>    * 若，gt_max_IoU < min_iou_thr，不做任何操作；

经过以上两步就将所有的anchor都分成了正样本和负样本还有ignore样本，其中被分为ignore样本的default bbox不参与loss的计算。

##### 4、RPN Loss

在网络的训练过程中，针对RPN层会产生两类 $loss$ ，分别为 $L_{cls}，L_{reg}$ 代表分类和回归的 $loss$ 。

* 针对 $L_{cls}$ 采用的是 $cross-entropy$ $ Loss$ (交叉熵损失)函数，定义如下:

$$
L_{cls}(p_i, p_i^*) = -[p_i\log{p_i^*}+(1-p_i)\log{(1-p_i^*)}]
$$

其中 $p_i$ 是指网络对于第 $i$ 个anchor的判定为前景框的概率；$p_i^*$ 是指为第 $i$ 个anchor分配的gt_label，为1代表该anchor为前景框，为0代表为背景框。

*  针对 $L_{reg}$ 采用的是 $smooth$ $L_1$ $Loss$ ，定义如下：

$$
L_{reg}(t_i, t_i^*) = 
\begin{cases}
0.5(t_i-t_i^*)^2,\quad|t_i-t_i^*|<1\\
|t_i-t_i^*|-0.5,\quad otherwise,
\end{cases}
$$

基于以上定义，给出RPN网络的最终 Loss 为：
$$
L(\{p_i\},\{t_i\}) = \frac{1}{N_{cls}}\sum\limits _i{L_{cls}(p_i, p_i^*)}\\+\lambda\frac{1}{N_{reg}}\sum\limits _i{p_i^*L_{reg}(t_i, t_i^*)}.
$$
其中$N_{cls}$, $N_{reg}$ 为平均化系数，$N_{cls}$ 表示参与计算loss的样本框个数，$N_{reg}$ 表示anchor位置的个数 (即feature map的面积)。

> 值得注意的是，在实际过程中，并不是所有的正负样本框的loss都会参与计算；在mmdetection中的faster rcnn配置文件中定义了这样一组参数：
>
> ```python
> sampler=dict(
>             type='RandomSampler',
>             # 正负样本框总个数
>             num=256,
>             # 正负样本框的比例（当总的正样本数>num*pos_fraction）
>             pos_fraction=0.5,
>             neg_pos_ub=-1,
>             add_gt_as_proposals=False)
> ```
>
> sampler表示对bboxes进行采样，采样的总数为num=256，其中正负样本框的比例为1:1，我们用num_pos、num_neg来分别代表正负样框的总数 ，则采样的规则如下：
>
> * 如果num_pos < pos_fraction * num，则将所有的正样本框都挑出来，再从所有的负样本框中随机挑出num-num_pos个负样本框。
> * 如果num_pos > pos_fraction * num，则从正样本框中随机挑出pos_fraction*num个正样本框，再从所有的负样本框中随机挑出 pos_fraction\*num个负样本框。
>
> 所以上述公式中的 $ N_{cls}$ = num。

##### 5、NMS (None Maximum Suppression)

为了保证ROI Pooling/Align阶段的处理速度，需要对所有的proposals (正样本框) 进行去重操作，即NMS(非极大值抑制)。首先看看配置文件中RPN阶段NMS的参数:

```python
rpn_proposal=dict(
        nms_across_levels=False,
        nms_pre=2000,
        nms_post=2000,
        max_num=300,
        nms_thr=0.7,
        min_bbox_size=0)
```

在论文中对于NMS的介绍如下：

![](/home/vansikey/Pictures/NMS.png)

论文里说留下2000个proposal，但实际上proposal的真正数量为max_num=300。

#### ROI extractor

在RPN网络得到将近300个proposals之后，还需要对每个proposal内的物体进行进一步的分类和对每个proposal的框进行精修，这一过程就交由ROI extractor层去做。

在faster R-CNN中通过使用ROI Pooling将每个proposal映射成为7×7的feature map，ROI Pooling layer的实现了将Input：(300个proposal：[300, x1, y1, x2, y2]，feature_map: [512, H, W])-------->output: (max pooling之后的feature map：[300, 512, 7, 7])，以此输出作为下一层的全连接的输入。

#### FCBBoxHead

我们看一下配置文件：

```
bbox_head=dict(
        type='SharedFCBBoxHead',
        num_fcs=2,
        in_channels=512,
        fc_out_channels=4096,
        roi_feat_size=7,
        num_classes=81,
        target_means=[0., 0., 0., 0.],
        target_stds=[0.1, 0.1, 0.2, 0.2],
        reg_class_agnostic=False,
        loss_cls=dict(
            type='CrossEntropyLoss',
            use_sigmoid=False,
            loss_weight=1.0),
        loss_bbox=dict(
            type='SmoothL1Loss', beta=1.0, loss_weight=1.0)))
```

经过ROI Pooling Layer之后，batch_size=300，proposal feature map的大小变成了 $7\times 7\times 512$，对该特征图进行两层全链接，如下图所示：

![img](https://images2018.cnblogs.com/blog/75922/201803/75922-20180306114353141-33590742.png) 

最后同样利用Softmax Loss和smooth L1 Loss完成分类和定位。

在计算第一个FC之前，先将ROI Pooling的输出的张量flatten成 [batch_size, 512\*7\*7]，然后经过一层全连接层实现512×7×7-d到4096-d的转换，最后在通过两个并行的全连接层实现：

* 对region proposals进行bounding box regression，获取更高精度的bounding box，
* 通过全连接和softmax对region proposals进行具体类别的分类。

在该层中同样会产生两个loss，计算的方式与RPN中的类似就不多做赘述。最后得到300精修后的proposals和其对应的类别标签，同样需要经过NMS去重，使得每个物体只由一个bounding box去框住。

